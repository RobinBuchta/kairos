{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/buchta/anaconda3/envs/kairos/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.data import *\n",
    "import networkx as nx\n",
    "\n",
    "import hashlib\n",
    "def stringtomd5(originstr):\n",
    "    originstr = originstr.encode(\"utf-8\")\n",
    "    signaturemd5 = hashlib.sha256()\n",
    "    signaturemd5.update(originstr)\n",
    "    return signaturemd5.hexdigest()\n",
    "filePath=\"/mnt/trustdatastore/datasets/darpa_engagement_3/data/theia/unpacked/\"\n",
    "\n",
    "filelist = os.listdir(filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ta1-theia-e3-official-1r.json',\n",
       " 'ta1-theia-e3-official-1r.json.1',\n",
       " 'ta1-theia-e3-official-1r.json.2',\n",
       " 'ta1-theia-e3-official-1r.json.3',\n",
       " 'ta1-theia-e3-official-1r.json.4',\n",
       " 'ta1-theia-e3-official-1r.json.5',\n",
       " 'ta1-theia-e3-official-1r.json.6',\n",
       " 'ta1-theia-e3-official-1r.json.7',\n",
       " 'ta1-theia-e3-official-1r.json.8',\n",
       " 'ta1-theia-e3-official-1r.json.9',\n",
       " 'ta1-theia-e3-official-3.json',\n",
       " 'ta1-theia-e3-official-5m.json',\n",
       " 'ta1-theia-e3-official-6r.json',\n",
       " 'ta1-theia-e3-official-6r.json.1',\n",
       " 'ta1-theia-e3-official-6r.json.10',\n",
       " 'ta1-theia-e3-official-6r.json.11',\n",
       " 'ta1-theia-e3-official-6r.json.12',\n",
       " 'ta1-theia-e3-official-6r.json.2',\n",
       " 'ta1-theia-e3-official-6r.json.3',\n",
       " 'ta1-theia-e3-official-6r.json.4',\n",
       " 'ta1-theia-e3-official-6r.json.5',\n",
       " 'ta1-theia-e3-official-6r.json.6',\n",
       " 'ta1-theia-e3-official-6r.json.7',\n",
       " 'ta1-theia-e3-official-6r.json.8',\n",
       " 'ta1-theia-e3-official-6r.json.9']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(filelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filelist=[\n",
    "#     'ta1-theia-e3-official-1r.json',\n",
    "#     'ta1-theia-e3-official-1r.json.1',\n",
    "#     'ta1-theia-e3-official-1r.json.2',\n",
    "#     'ta1-theia-e3-official-1r.json.3',\n",
    "#     'ta1-theia-e3-official-1r.json.4',\n",
    "#     'ta1-theia-e3-official-1r.json.5',\n",
    "#     'ta1-theia-e3-official-1r.json.6',\n",
    "#     'ta1-theia-e3-official-1r.json.7',\n",
    "#     'ta1-theia-e3-official-1r.json.8',\n",
    "#     'ta1-theia-e3-official-1r.json.9',\n",
    "#     'ta1-theia-e3-official-3.json',\n",
    "#     'ta1-theia-e3-official-5m.json',\n",
    "#     'ta1-theia-e3-official-6r.json',\n",
    "#     'ta1-theia-e3-official-6r.json.1',\n",
    "#     'ta1-theia-e3-official-6r.json.2',\n",
    "#     'ta1-theia-e3-official-6r.json.3',\n",
    "#     'ta1-theia-e3-official-6r.json.4',\n",
    "#     'ta1-theia-e3-official-6r.json.5',\n",
    "#     'ta1-theia-e3-official-6r.json.6',\n",
    "#     'ta1-theia-e3-official-6r.json.7',\n",
    "#     'ta1-theia-e3-official-6r.json.8',\n",
    "#     'ta1-theia-e3-official-6r.json.9',\n",
    "#     'ta1-theia-e3-official-6r.json.10',\n",
    "#     'ta1-theia-e3-official-6r.json.11',\n",
    "#     'ta1-theia-e3-official-6r.json.12'\n",
    "# ]\n",
    "filelist = ['ta1-theia-e3-official-6r.json.9',\n",
    " 'ta1-theia-e3-official-1r.json.9',\n",
    " 'ta1-theia-e3-official-6r.json.8',\n",
    " 'ta1-theia-e3-official-6r.json.12',\n",
    " 'ta1-theia-e3-official-1r.json.7',\n",
    " 'ta1-theia-e3-official-6r.json.7',\n",
    " 'ta1-theia-e3-official-6r.json.5',\n",
    " 'ta1-theia-e3-official-1r.json.3',\n",
    " 'ta1-theia-e3-official-6r.json',\n",
    " 'ta1-theia-e3-official-1r.json.5',\n",
    " 'ta1-theia-e3-official-6r.json.11',\n",
    " 'ta1-theia-e3-official-1r.json.4',\n",
    " 'ta1-theia-e3-official-1r.json.6',\n",
    " 'ta1-theia-e3-official-5m.json',\n",
    " 'ta1-theia-e3-official-1r.json.2',\n",
    " 'ta1-theia-e3-official-6r.json.10',\n",
    " 'ta1-theia-e3-official-6r.json.4',\n",
    " 'ta1-theia-e3-official-6r.json.1',\n",
    " 'ta1-theia-e3-official-3.json',\n",
    " 'ta1-theia-e3-official-1r.json.8',\n",
    " 'ta1-theia-e3-official-1r.json.1',\n",
    " 'ta1-theia-e3-official-6r.json.6',\n",
    " 'ta1-theia-e3-official-6r.json.2',\n",
    " 'ta1-theia-e3-official-6r.json.3',\n",
    " 'ta1-theia-e3-official-1r.json']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "import time\n",
    "import pytz\n",
    "from time import mktime\n",
    "from datetime import datetime\n",
    "import time\n",
    "def ns_time_to_datetime(ns):\n",
    "    \"\"\"\n",
    "    :param ns: int nano timestamp\n",
    "    :return: datetime   format: 2013-10-10 23:40:00.000000000\n",
    "    \"\"\"\n",
    "    dt = datetime.fromtimestamp(int(ns) // 1000000000)\n",
    "    s = dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    s += '.' + str(int(int(ns) % 1000000000)).zfill(9)\n",
    "    return s\n",
    "\n",
    "def ns_time_to_datetime_US(ns):\n",
    "    \"\"\"\n",
    "    :param ns: int nano timestamp\n",
    "    :return: datetime   format: 2013-10-10 23:40:00.000000000\n",
    "    \"\"\"\n",
    "    tz = pytz.timezone('US/Eastern')\n",
    "    dt = pytz.datetime.datetime.fromtimestamp(int(ns) // 1000000000, tz)\n",
    "    s = dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    s += '.' + str(int(int(ns) % 1000000000)).zfill(9)\n",
    "    return s\n",
    "\n",
    "def time_to_datetime_US(s):\n",
    "    \"\"\"\n",
    "    :param ns: int nano timestamp\n",
    "    :return: datetime   format: 2013-10-10 23:40:00\n",
    "    \"\"\"\n",
    "    tz = pytz.timezone('US/Eastern')\n",
    "    dt = pytz.datetime.datetime.fromtimestamp(int(s), tz)\n",
    "    s = dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    return s\n",
    "\n",
    "def datetime_to_ns_time(date):\n",
    "    \"\"\"\n",
    "    :param date: str   format: %Y-%m-%d %H:%M:%S   e.g. 2013-10-10 23:40:00\n",
    "    :return: nano timestamp\n",
    "    \"\"\"\n",
    "    timeArray = time.strptime(date, \"%Y-%m-%d %H:%M:%S\")\n",
    "    timeStamp = int(time.mktime(timeArray))\n",
    "    timeStamp = timeStamp * 1000000000\n",
    "    return timeStamp\n",
    "\n",
    "def datetime_to_ns_time_US(date):\n",
    "    \"\"\"\n",
    "    :param date: str   format: %Y-%m-%d %H:%M:%S   e.g. 2013-10-10 23:40:00\n",
    "    :return: nano timestamp\n",
    "    \"\"\"\n",
    "    tz = pytz.timezone('US/Eastern')\n",
    "    timeArray = time.strptime(date, \"%Y-%m-%d %H:%M:%S\")\n",
    "    dt = datetime.fromtimestamp(mktime(timeArray))\n",
    "    timestamp = tz.localize(dt)\n",
    "    timestamp = timestamp.timestamp()\n",
    "    timeStamp = timestamp * 1000000000\n",
    "    return int(timeStamp)\n",
    "\n",
    "def datetime_to_timestamp_US(date):\n",
    "    \"\"\"\n",
    "    :param date: str   format: %Y-%m-%d %H:%M:%S   e.g. 2013-10-10 23:40:00\n",
    "    :return: nano timestamp\n",
    "    \"\"\"\n",
    "    tz = pytz.timezone('US/Eastern')\n",
    "    timeArray = time.strptime(date, \"%Y-%m-%d %H:%M:%S\")\n",
    "    dt = datetime.fromtimestamp(mktime(timeArray))\n",
    "    timestamp = tz.localize(dt)\n",
    "    timestamp = timestamp.timestamp()\n",
    "    timeStamp = timestamp\n",
    "    return int(timeStamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database setting (Make sure the database and tables are created)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "from psycopg2 import extras as ex\n",
    "connect = psycopg2.connect(database = 'tc_theia_dataset_db',\n",
    "                           host = '141.71.31.196',\n",
    "                           user = 'kairos',\n",
    "                           password = 'lolroflxd',\n",
    "                           port = '54321'\n",
    "                          )\n",
    "\n",
    "cur = connect.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Netflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|█████████████████████████████████████▏                                                                                                                     | 6/25 [09:05<30:35, 96.61s/it]"
     ]
    }
   ],
   "source": [
    "\n",
    "netobjset=set()\n",
    "netobj2hash={}# \n",
    "datalist=[]\n",
    "for file in tqdm(filelist):\n",
    "        with open(filePath + file, \"r\") as f:\n",
    "            for line in f:\n",
    "#                 pass\n",
    "                if '{\"datum\":{\"com.bbn.tc.schema.avro.cdm18.NetFlowObject\"' in line :\n",
    "            \n",
    "                    try:\n",
    "                        res=re.findall('NetFlowObject\":{\"uuid\":\"(.*?)\"(.*?)\"localAddress\":\"(.*?)\",\"localPort\":(.*?),\"remoteAddress\":\"(.*?)\",\"remotePort\":(.*?),',line)[0]\n",
    "\n",
    "                        nodeid=res[0]\n",
    "                        srcaddr=res[2]\n",
    "                        srcport=res[3]\n",
    "                        dstaddr=res[4]\n",
    "                        dstport=res[5]\n",
    "\n",
    "                        nodeproperty=srcaddr+\",\"+srcport+\",\"+dstaddr+\",\"+dstport \n",
    "                        hashstr=stringtomd5(nodeproperty)\n",
    "                        netobj2hash[nodeid]=[hashstr,nodeproperty]\n",
    "                        netobj2hash[hashstr]=nodeid\n",
    "                        netobjset.add(hashstr)\n",
    "                    except:\n",
    "                        \n",
    "                        pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalist=[]\n",
    "for i in netobj2hash.keys():\n",
    "    if len(i)!=64:\n",
    "        datalist.append([i]+[netobj2hash[i][0]]+netobj2hash[i][1].split(\",\"))\n",
    "\n",
    "\n",
    "\n",
    "sql = '''insert into netflow_node_table\n",
    "                     values %s\n",
    "        '''\n",
    "ex.execute_values(cur,sql, datalist,page_size=10000)\n",
    "connect.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scusess_count=0\n",
    "fail_count=0\n",
    "subject_obj2hash={}# \n",
    "\n",
    "subjectset=set()\n",
    "subject2hash={}\n",
    "\n",
    "datalist=[]\n",
    "for file in tqdm(filelist):\n",
    "        with open(filePath + file, \"r\") as f:\n",
    "            for line in f:\n",
    "                if '{\"datum\":{\"com.bbn.tc.schema.avro.cdm18.Subject\"' in line :\n",
    "                    res=re.findall('Subject\":{\"uuid\":\"(.*?)\"(.*?)\"cmdLine\":{\"string\":\"(.*?)\"}(.*?)\"properties\":{\"map\":{\"tgid\":\"(.*?)\"',line)[0]\n",
    "                    try:\n",
    "                        path_str=re.findall('\"path\":\"(.*?)\"',line)[0] \n",
    "                        path=path_str\n",
    "                    except:\n",
    "                        path=\"null\"\n",
    "                    nodeid=res[0]\n",
    "                    cmdLine=res[2]\n",
    "                    tgid=res[4]\n",
    "                    \n",
    "                    nodeproperty=cmdLine+\",\"+tgid+\",\"+path\n",
    "#                     if len(cmdLine) > 1000:\n",
    "#                         print(file)\n",
    "#                         print(cmdLine)\n",
    "#                         print(line)\n",
    "#                         input()\n",
    "                    \n",
    "                    hashstr=stringtomd5(nodeproperty)\n",
    "                    subject2hash[nodeid]=[hashstr,cmdLine,tgid,path]\n",
    "                    subject2hash[hashstr]=nodeid\n",
    "                    subjectset.add(hashstr)\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subject2hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "datalist=[]\n",
    "for i in subject2hash.keys():\n",
    "    if len(i)!=64:\n",
    "        datalist.append([i]+subject2hash[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalist[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(datalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sql = '''insert into subject_node_table\n",
    "                     values %s\n",
    "        '''\n",
    "ex.execute_values(cur,sql, datalist,page_size=10000)\n",
    "connect.commit()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileset=set()\n",
    "\n",
    "# Remove the file nodes whose paths are null. So when parsing the edges, \n",
    "# if the src or dst node is file and it doesn't have pathname, then we can ignore them.\n",
    "\n",
    "file2hash={}  \n",
    "nullcount=0\n",
    "\n",
    "\n",
    "for file in tqdm(filelist):\n",
    "        with open(filePath + file, \"r\") as f:\n",
    "            for line in f:\n",
    "                if '{\"datum\":{\"com.bbn.tc.schema.avro.cdm18.FileObject\"' in line :\n",
    "                    try:\n",
    "\n",
    "                        res=re.findall('FileObject\":{\"uuid\":\"(.*?)\"(.*?)\"filename\":\"(.*?)\"',line)[0]\n",
    "                        nodeid=res[0]\n",
    "                        filepath=res[2]\n",
    "                        nodeproperty=filepath\n",
    "                        hashstr=stringtomd5(nodeproperty)\n",
    "                        file2hash[nodeid]=[hashstr,nodeproperty]\n",
    "                        file2hash[hashstr]=nodeid\n",
    "                        fileset.add(hashstr)\n",
    "                    except:\n",
    "                        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalist=[]\n",
    "for i in file2hash.keys():\n",
    "    if len(i)!=64:\n",
    "        datalist.append([i]+file2hash[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''insert into file_node_table\n",
    "                     values %s\n",
    "        '''\n",
    "ex.execute_values(cur,sql, datalist,page_size=10000)\n",
    "connect.commit()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the event data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate the data for node2id table\n",
    "node_list={}\n",
    "##################################################################################################\n",
    "sql=\"\"\"\n",
    "select * from file_node_table;\n",
    "\"\"\"\n",
    "cur.execute(sql)\n",
    "records = cur.fetchall()\n",
    "\n",
    "for i in records:    \n",
    "    node_list[i[1]]=[\"file\",i[-1]]\n",
    "\n",
    "file_uuid2hash={}\n",
    "for i in records:\n",
    "    file_uuid2hash[i[0]]=i[1]\n",
    "##################################################################################################    \n",
    "sql=\"\"\"\n",
    "select * from subject_node_table;\n",
    "\"\"\"\n",
    "cur.execute(sql)\n",
    "records = cur.fetchall()\n",
    "\n",
    "for i in records:\n",
    "    node_list[i[1]]=[\"subject\",i[-1]]\n",
    "\n",
    "subject_uuid2hash={}\n",
    "for i in records:\n",
    "    subject_uuid2hash[i[0]]=i[1]\n",
    "##################################################################################################\n",
    "sql=\"\"\"\n",
    "select * from netflow_node_table;\n",
    "\"\"\"\n",
    "cur.execute(sql)\n",
    "records = cur.fetchall()\n",
    "\n",
    "for i in records:\n",
    "    \n",
    "    node_list[i[1]]=[\"netflow\",i[-2]+\":\"+i[-1]]\n",
    "\n",
    "net_uuid2hash={}\n",
    "for i in records:\n",
    "    net_uuid2hash[i[0]]=i[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_list_database=[]\n",
    "node_index=0\n",
    "for i in node_list:\n",
    "    node_list_database.append([i]+node_list[i]+[node_index])\n",
    "    node_index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''insert into node2id\n",
    "                     values %s\n",
    "        '''\n",
    "ex.execute_values(cur,sql, node_list_database,page_size=10000)\n",
    "connect.commit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing the map for nodeid to msg\n",
    "sql=\"select * from node2id ORDER BY index_id;\"\n",
    "cur.execute(sql)\n",
    "rows = cur.fetchall()\n",
    "\n",
    "nodeid2msg={}  # nodeid => msg and node hash => nodeid\n",
    "for i in rows:\n",
    "    nodeid2msg[i[0]]=i[-1]\n",
    "    nodeid2msg[i[-1]]={i[1]:i[2]} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nodeid2msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "datalist=[]\n",
    "\n",
    "\n",
    "# firsttime=1522764134000000000\n",
    "\n",
    "for file in tqdm(filelist): \n",
    "        with open(filePath + file, \"r\") as f: \n",
    "            for line in f: \n",
    "                if '{\"datum\":{\"com.bbn.tc.schema.avro.cdm18.Event\"' in line and \"EVENT_FLOWS_TO\" not in line:\n",
    "                    time=re.findall('\"timestampNanos\":(.*?),',line)[0]\n",
    "                    time=int(time)\n",
    "                    subjectid=re.findall('\"subject\":{\"com.bbn.tc.schema.avro.cdm18.UUID\":\"(.*?)\"',line)[0]\n",
    "                    objectid=re.findall('\"predicateObject\":{\"com.bbn.tc.schema.avro.cdm18.UUID\":\"(.*?)\"',line)[0]\n",
    "                    relation_type=re.findall('\"type\":\"(.*?)\"',line)[0]\n",
    "\n",
    "\n",
    "                    if subjectid in subject2hash.keys():\n",
    "                        # this subject is recorded\n",
    "                        subjectid=subject2hash[subjectid][0] # convert uuid to hash value\n",
    "                    if objectid in subject2hash.keys():\n",
    "                        objectid=subject2hash[objectid][0] # convert uuid to hash value\n",
    "                    if objectid in file2hash.keys():\n",
    "                        objectid=file2hash[objectid][0] # convert uuid to hash value\n",
    "                    if objectid in netobj2hash.keys():\n",
    "                        objectid=netobj2hash[objectid][0] # convert uuid to hash value\n",
    "                    if len(subjectid)==64 and len(objectid) == 64:\n",
    "                        # Only when both src and dst UUIDs are converted into hashid will this edge be vaild and stored.\n",
    "                        if relation_type in ['EVENT_READ','EVENT_READ_SOCKET_PARAMS','EVENT_RECVFROM','EVENT_RECVMSG']:\n",
    "                            # data flows to subject, so process is dst node of the event\n",
    "                            datalist.append((objectid,nodeid2msg[objectid],relation_type,subjectid,nodeid2msg[subjectid],time))\n",
    "                        else:\n",
    "                            datalist.append((subjectid,nodeid2msg[subjectid],relation_type,objectid,nodeid2msg[objectid],time))\n",
    "\n",
    "                     \n",
    "                        \n",
    "                        if len(datalist)>=10000:\n",
    "                            try:                \n",
    "                                sql = '''insert into event_table\n",
    "                                                     values %s\n",
    "                                        '''\n",
    "                                ex.execute_values(cur,sql, datalist,page_size=100000)\n",
    "                                connect.commit()  \n",
    "                                datalist=[]\n",
    "                            except:\n",
    "                                print(\"Failed to wirte database！\")\n",
    "                                connect.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featurization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "\n",
    "from sklearn import preprocessing \n",
    "import numpy as np\n",
    "# \n",
    "\n",
    "FH_string=FeatureHasher(n_features=16,input_type=\"string\")\n",
    "FH_dict=FeatureHasher(n_features=16,input_type=\"dict\")\n",
    "\n",
    "\n",
    "def path2higlist(p):\n",
    "    l=[]\n",
    "    spl=p.strip().split('/')\n",
    "    for i in spl:\n",
    "        if len(l)!=0:\n",
    "            l.append(l[-1]+'/'+i)\n",
    "        else:\n",
    "            l.append(i)\n",
    "#     print(l)\n",
    "    return l\n",
    "\n",
    "def ip2higlist(p):\n",
    "    l=[]\n",
    "    spl=p.strip().split('.')\n",
    "    for i in spl:\n",
    "        if len(l)!=0:\n",
    "            l.append(l[-1]+'.'+i)\n",
    "        else:\n",
    "            l.append(i)\n",
    "#     print(l)\n",
    "    return l\n",
    "def list2str(l):\n",
    "    s=''\n",
    "    for i in l:\n",
    "        s+=i\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_msg_vec=[]\n",
    "node_msg_dic_list=[]\n",
    "for i in tqdm(nodeid2msg.keys()):\n",
    "    if type(i)==int:\n",
    "        if 'netflow' in nodeid2msg[i].keys():\n",
    "            higlist=['netflow']\n",
    "            higlist+=ip2higlist(nodeid2msg[i]['netflow'])\n",
    "            \n",
    "        if 'file' in nodeid2msg[i].keys():\n",
    "            higlist=['file']\n",
    "            higlist+=path2higlist(nodeid2msg[i]['file'])\n",
    "            \n",
    "#             print(higlist)\n",
    "        if 'subject' in nodeid2msg[i].keys():\n",
    "            higlist=['subject']\n",
    "            higlist+=path2higlist(nodeid2msg[i]['subject'])\n",
    "        node_msg_dic_list.append(list2str(higlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node2higvec=[]\n",
    "for i in tqdm(node_msg_dic_list):\n",
    "    vec=FH_string.transform([i]).toarray()\n",
    "    node2higvec.append(vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node2higvec=np.array(node2higvec).reshape([-1,16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel2id={1: 'EVENT_CONNECT',\n",
    " 'EVENT_CONNECT': 1,\n",
    " 2: 'EVENT_EXECUTE',\n",
    " 'EVENT_EXECUTE': 2,\n",
    " 3: 'EVENT_OPEN',\n",
    " 'EVENT_OPEN': 3,\n",
    " 4: 'EVENT_READ',\n",
    " 'EVENT_READ': 4,\n",
    " 5: 'EVENT_RECVFROM',\n",
    " 'EVENT_RECVFROM': 5,\n",
    " 6: 'EVENT_RECVMSG',\n",
    " 'EVENT_RECVMSG': 6,\n",
    " 7: 'EVENT_SENDMSG',\n",
    " 'EVENT_SENDMSG': 7,\n",
    " 8: 'EVENT_SENDTO',\n",
    " 'EVENT_SENDTO': 8,\n",
    " 9: 'EVENT_WRITE',\n",
    " 'EVENT_WRITE': 9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geneate edge type one-hot\n",
    "relvec=torch.nn.functional.one_hot(torch.arange(0, len(rel2id.keys())//2), num_classes=len(rel2id.keys())//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map different relation types to their one-hot encoding\n",
    "rel2vec={}\n",
    "for i in rel2id.keys():\n",
    "    if type(i) is not int:\n",
    "        rel2vec[i]= relvec[rel2id[i]-1]\n",
    "        rel2vec[relvec[rel2id[i]-1]]=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the results\n",
    "torch.save(node2higvec,\"node2higvec\")\n",
    "torch.save(rel2vec,\"rel2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node2higvec=torch.load(\"./node2higvec\")\n",
    "rel2vec=torch.load(\"./rel2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"mkdir -p ./train_graph/\")\n",
    "for day in tqdm(range(2,14)):\n",
    "    start_timestamp=datetime_to_ns_time_US('2018-04-'+str(day)+' 00:00:00')\n",
    "    end_timestamp=datetime_to_ns_time_US('2018-04-'+str(day+1)+' 00:00:00')\n",
    "    sql=\"\"\"\n",
    "    select * from event_table\n",
    "    where\n",
    "          timestamp_rec>'%s' and timestamp_rec<'%s'\n",
    "           ORDER BY timestamp_rec;\n",
    "    \"\"\"%(start_timestamp,end_timestamp)\n",
    "    cur.execute(sql)\n",
    "    events = cur.fetchall()\n",
    "    print('2018-04-'+str(day),\" events count:\",str(len(events)))\n",
    "    edge_list=[]\n",
    "    for e in events:\n",
    "        edge_temp=[int(e[1]),int(e[4]),e[2],e[5]]\n",
    "        if e[2] in rel2id:\n",
    "            edge_list.append(edge_temp)\n",
    "    print('2018-04-'+str(day),\" edge list len:\",str(len(edge_list)))\n",
    "    \n",
    "    if len(edge_list) == 0:\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    dataset = TemporalData()\n",
    "    src = []\n",
    "    dst = []\n",
    "    msg = []\n",
    "    t = []\n",
    "    for i in edge_list:\n",
    "        src.append(int(i[0]))\n",
    "        dst.append(int(i[1]))\n",
    "    #     msg.append(torch.cat([torch.from_numpy(node2higvec_bn[i[0]]), rel2vec[i[2]], torch.from_numpy(node2higvec_bn[i[1]])] ))\n",
    "        msg.append(torch.cat([torch.from_numpy(node2higvec[i[0]]), rel2vec[i[2]], torch.from_numpy(node2higvec[i[1]])] ))\n",
    "        t.append(int(i[3]))\n",
    "\n",
    "    dataset.src = torch.tensor(src)\n",
    "    dataset.dst = torch.tensor(dst)\n",
    "    dataset.t = torch.tensor(t)\n",
    "    dataset.msg = torch.vstack(msg)\n",
    "    dataset.src = dataset.src.to(torch.long)\n",
    "    dataset.dst = dataset.dst.to(torch.long)\n",
    "    dataset.msg = dataset.msg.to(torch.float)\n",
    "    dataset.t = dataset.t.to(torch.long)\n",
    "    torch.save(dataset, \"./train_graph/graph_4_\"+str(day)+\".TemporalData.simple\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kairos",
   "language": "python",
   "name": "kairos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "332.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
