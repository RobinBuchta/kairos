{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/buchta/anaconda3/envs/kairos/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.data import *\n",
    "import networkx as nx\n",
    "\n",
    "import hashlib\n",
    "def stringtomd5(originstr):\n",
    "    originstr = originstr.encode(\"utf-8\")\n",
    "    signaturemd5 = hashlib.sha256()\n",
    "    signaturemd5.update(originstr)\n",
    "    return signaturemd5.hexdigest()\n",
    "\n",
    "filePath=\"/mnt/trustdatastore/datasets/darpa_engagement_3/data/clearscope/unpacked/\"\n",
    "\n",
    "filelist = os.listdir(filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist = ['ta1-clearscope-e3-official-1.json',\n",
    " 'ta1-clearscope-e3-official-1.json.1',\n",
    " 'ta1-clearscope-e3-official-1.json.10',\n",
    " 'ta1-clearscope-e3-official-1.json.11',\n",
    " 'ta1-clearscope-e3-official-1.json.12',\n",
    " 'ta1-clearscope-e3-official-1.json.13',\n",
    " 'ta1-clearscope-e3-official-1.json.14',\n",
    " 'ta1-clearscope-e3-official-1.json.15',\n",
    " 'ta1-clearscope-e3-official-1.json.16',\n",
    " 'ta1-clearscope-e3-official-1.json.17',\n",
    " 'ta1-clearscope-e3-official-1.json.18',\n",
    " 'ta1-clearscope-e3-official-1.json.19',\n",
    " 'ta1-clearscope-e3-official-1.json.2',\n",
    " 'ta1-clearscope-e3-official-1.json.3',\n",
    " 'ta1-clearscope-e3-official-1.json.4',\n",
    " 'ta1-clearscope-e3-official-1.json.5',\n",
    " 'ta1-clearscope-e3-official-1.json.6',\n",
    " 'ta1-clearscope-e3-official-1.json.7',\n",
    " 'ta1-clearscope-e3-official-1.json.8',\n",
    " 'ta1-clearscope-e3-official-1.json.9',\n",
    " 'ta1-clearscope-e3-official-2.json',\n",
    " 'ta1-clearscope-e3-official-2.json.1',\n",
    " 'ta1-clearscope-e3-official-2.json.10',\n",
    " 'ta1-clearscope-e3-official-2.json.11',\n",
    " 'ta1-clearscope-e3-official-2.json.12',\n",
    " 'ta1-clearscope-e3-official-2.json.13',\n",
    " 'ta1-clearscope-e3-official-2.json.14',\n",
    " 'ta1-clearscope-e3-official-2.json.15',\n",
    " 'ta1-clearscope-e3-official-2.json.16',\n",
    " 'ta1-clearscope-e3-official-2.json.17',\n",
    " 'ta1-clearscope-e3-official-2.json.18',\n",
    " 'ta1-clearscope-e3-official-2.json.19',\n",
    " 'ta1-clearscope-e3-official-2.json.2',\n",
    " 'ta1-clearscope-e3-official-2.json.20',\n",
    " 'ta1-clearscope-e3-official-2.json.21',\n",
    " 'ta1-clearscope-e3-official-2.json.22',\n",
    " 'ta1-clearscope-e3-official-2.json.23',\n",
    " 'ta1-clearscope-e3-official-2.json.24',\n",
    " 'ta1-clearscope-e3-official-2.json.25',\n",
    " 'ta1-clearscope-e3-official-2.json.26',\n",
    " 'ta1-clearscope-e3-official-2.json.27',\n",
    " 'ta1-clearscope-e3-official-2.json.28',\n",
    " 'ta1-clearscope-e3-official-2.json.3',\n",
    " 'ta1-clearscope-e3-official-2.json.4',\n",
    " 'ta1-clearscope-e3-official-2.json.5',\n",
    " 'ta1-clearscope-e3-official-2.json.6',\n",
    " 'ta1-clearscope-e3-official-2.json.7',\n",
    " 'ta1-clearscope-e3-official-2.json.8',\n",
    " 'ta1-clearscope-e3-official-2.json.9',\n",
    " 'ta1-clearscope-e3-official.json',\n",
    " 'ta1-clearscope-e3-official.json.1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "import time\n",
    "import pytz\n",
    "from time import mktime\n",
    "from datetime import datetime\n",
    "import time\n",
    "def ns_time_to_datetime(ns):\n",
    "    \"\"\"\n",
    "    :param ns: int nano timestamp\n",
    "    :return: datetime   format: 2013-10-10 23:40:00.000000000\n",
    "    \"\"\"\n",
    "    dt = datetime.fromtimestamp(int(ns) // 1000000000)\n",
    "    s = dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    s += '.' + str(int(int(ns) % 1000000000)).zfill(9)\n",
    "    return s\n",
    "\n",
    "def ns_time_to_datetime_US(ns):\n",
    "    \"\"\"\n",
    "    :param ns: int nano timestamp\n",
    "    :return: datetime   format: 2013-10-10 23:40:00.000000000\n",
    "    \"\"\"\n",
    "    tz = pytz.timezone('US/Eastern')\n",
    "    dt = pytz.datetime.datetime.fromtimestamp(int(ns) // 1000000000, tz)\n",
    "    s = dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    s += '.' + str(int(int(ns) % 1000000000)).zfill(9)\n",
    "    return s\n",
    "\n",
    "def time_to_datetime_US(s):\n",
    "    \"\"\"\n",
    "    :param ns: int nano timestamp\n",
    "    :return: datetime   format: 2013-10-10 23:40:00\n",
    "    \"\"\"\n",
    "    tz = pytz.timezone('US/Eastern')\n",
    "    dt = pytz.datetime.datetime.fromtimestamp(int(s), tz)\n",
    "    s = dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    return s\n",
    "\n",
    "def datetime_to_ns_time(date):\n",
    "    \"\"\"\n",
    "    :param date: str   format: %Y-%m-%d %H:%M:%S   e.g. 2013-10-10 23:40:00\n",
    "    :return: nano timestamp\n",
    "    \"\"\"\n",
    "    timeArray = time.strptime(date, \"%Y-%m-%d %H:%M:%S\")\n",
    "    timeStamp = int(time.mktime(timeArray))\n",
    "    timeStamp = timeStamp * 1000000000\n",
    "    return timeStamp\n",
    "\n",
    "def datetime_to_ns_time_US(date):\n",
    "    \"\"\"\n",
    "    :param date: str   format: %Y-%m-%d %H:%M:%S   e.g. 2013-10-10 23:40:00\n",
    "    :return: nano timestamp\n",
    "    \"\"\"\n",
    "    tz = pytz.timezone('US/Eastern')\n",
    "    timeArray = time.strptime(date, \"%Y-%m-%d %H:%M:%S\")\n",
    "    dt = datetime.fromtimestamp(mktime(timeArray))\n",
    "    timestamp = tz.localize(dt)\n",
    "    timestamp = timestamp.timestamp()\n",
    "    timeStamp = timestamp * 1000000000\n",
    "    return int(timeStamp)\n",
    "\n",
    "def datetime_to_timestamp_US(date):\n",
    "    \"\"\"\n",
    "    :param date: str   format: %Y-%m-%d %H:%M:%S   e.g. 2013-10-10 23:40:00\n",
    "    :return: nano timestamp\n",
    "    \"\"\"\n",
    "    tz = pytz.timezone('US/Eastern')\n",
    "    timeArray = time.strptime(date, \"%Y-%m-%d %H:%M:%S\")\n",
    "    dt = datetime.fromtimestamp(mktime(timeArray))\n",
    "    timestamp = tz.localize(dt)\n",
    "    timestamp = timestamp.timestamp()\n",
    "    timeStamp = timestamp\n",
    "    return int(timeStamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database setting (Make sure the database and tables are created)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "from psycopg2 import extras as ex\n",
    "connect = psycopg2.connect(database = 'tc_clearscope3_dataset_db',\n",
    "                           host = '141.71.31.196',\n",
    "                           user = 'kairos',\n",
    "                           password = 'lolroflxd',\n",
    "                           port = '54321'\n",
    "                          )\n",
    "\n",
    "cur = connect.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Netflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|███████████▉                                                                                                                                            | 4/51 [13:02<2:48:29, 215.10s/it]"
     ]
    }
   ],
   "source": [
    "netobjset=set()\n",
    "netobj2hash={}# \n",
    "datalist=[]\n",
    "for file in tqdm(filelist):\n",
    "        with open(filePath + file, \"r\") as f:\n",
    "            for line in f:\n",
    "#                 pass\n",
    "                if \"NetFlowObject\" in line:\n",
    "#                     print(line)\n",
    "                    try:\n",
    "                        res=re.findall('NetFlowObject\":{\"uuid\":\"(.*?)\"(.*?)\"localAddress\":\"(.*?)\",\"localPort\":(.*?),\"remoteAddress\":\"(.*?)\",\"remotePort\":(.*?),',line)[0]\n",
    "\n",
    "                        nodeid=res[0]\n",
    "                        srcaddr=res[2]\n",
    "                        srcport=res[3]\n",
    "                        dstaddr=res[4]\n",
    "                        dstport=res[5]\n",
    "\n",
    "                        nodeproperty=srcaddr+\",\"+srcport+\",\"+dstaddr+\",\"+dstport \n",
    "                        hashstr=stringtomd5(nodeproperty)\n",
    "                        netobj2hash[nodeid]=[hashstr,nodeproperty]\n",
    "                        netobj2hash[hashstr]=nodeid\n",
    "                        netobjset.add(hashstr)\n",
    "                    except:\n",
    "                        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalist=[]\n",
    "for i in netobj2hash.keys():\n",
    "    if len(i)!=64:\n",
    "        datalist.append([i]+[netobj2hash[i][0]]+netobj2hash[i][1].split(\",\"))\n",
    "\n",
    "\n",
    "sql = '''insert into netflow_node_table\n",
    "                     values %s\n",
    "        '''\n",
    "ex.execute_values(cur,sql, datalist,page_size=10000)\n",
    "connect.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scusess_count=0\n",
    "fail_count=0\n",
    "subject_objset=set()\n",
    "subject_obj2hash={}# \n",
    "for file in tqdm(filelist):\n",
    "        with open(filePath + file, \"r\") as f:\n",
    "#             for line in tqdm(f): \n",
    "            for line in (f):\n",
    "                if \"schema.avro.cdm18.Subject\" in line:\n",
    "#                     print(line)\n",
    "                    subject_uuid=re.findall('Subject\":{\"uuid\":\"(.*?)\",(.*?)\"cmdLine\":{\"string\":\"(.*?)\"}',line)\n",
    "#                     subject_uuid=re.findall('\"subject\":{\"com.bbn.tc.schema.avro.cdm18.UUID\":\"(.*?)\"}(.*?)\"exec\":\"(.*?)\"',line)\n",
    "                    try:\n",
    "#                         (subject_uuid[0][-1])\n",
    "                        subject_obj2hash[subject_uuid[0][0]]=subject_uuid[0][-1]\n",
    "                        scusess_count+=1\n",
    "                    except:\n",
    "                        try:\n",
    "                            subject_obj2hash[subject_uuid[0][0]]=\"null\"\n",
    "                        except:\n",
    "                            pass\n",
    "#                             print(line)\n",
    "#                         print(line)                        \n",
    "                        fail_count+=1\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalist=[]\n",
    "for i in subject_obj2hash.keys():\n",
    "    if len(i)!=64:\n",
    "        datalist.append([i]+[stringtomd5(subject_obj2hash[i]),subject_obj2hash[i]])\n",
    "\n",
    "sql = '''insert into subject_node_table\n",
    "                     values %s\n",
    "        '''\n",
    "ex.execute_values(cur,sql, datalist,page_size=10000)\n",
    "connect.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_node=set()\n",
    "file_obj2hash={}\n",
    "for file in tqdm(filelist):\n",
    "        with open(filePath + file, \"r\") as f:\n",
    "            for line in f:\n",
    "                if \"com.bbn.tc.schema.avro.cdm18.FileObject\" in line:\n",
    "#                     print(line)\n",
    "                    Object_uuid=re.findall('FileObject\":{\"uuid\":\"(.*?)\",(.*?)\"path\":\"(.*?)\"',line) \n",
    "                    try:\n",
    "                        file_node.add(Object_uuid[0])\n",
    "                        file_obj2hash[Object_uuid[0][0]]=Object_uuid[0][-1]\n",
    "                    except:\n",
    "                        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalist=[]\n",
    "for i in file_obj2hash.keys():\n",
    "    if len(i)!=64:\n",
    "        datalist.append([i]+[stringtomd5(file_obj2hash[i]),file_obj2hash[i]])\n",
    "\n",
    "sql = '''insert into file_node_table\n",
    "                     values %s\n",
    "        '''\n",
    "ex.execute_values(cur,sql, datalist,page_size=10000)\n",
    "connect.commit() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the event data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate the data for node2id table\n",
    "node_list={}\n",
    "##################################################################################################\n",
    "sql=\"\"\"\n",
    "select * from file_node_table;\n",
    "\"\"\"\n",
    "cur.execute(sql)\n",
    "records = cur.fetchall()\n",
    "\n",
    "for i in records:    \n",
    "    node_list[i[1]]=[\"file\",i[-1]]\n",
    "\n",
    "file_uuid2hash={}\n",
    "for i in records:\n",
    "    file_uuid2hash[i[0]]=i[1]\n",
    "##################################################################################################    \n",
    "sql=\"\"\"\n",
    "select * from subject_node_table;\n",
    "\"\"\"\n",
    "cur.execute(sql)\n",
    "records = cur.fetchall()\n",
    "\n",
    "for i in records:\n",
    "    node_list[i[1]]=[\"subject\",i[-1]]\n",
    "\n",
    "subject_uuid2hash={}\n",
    "for i in records:\n",
    "    subject_uuid2hash[i[0]]=i[1]\n",
    "##################################################################################################\n",
    "sql=\"\"\"\n",
    "select * from netflow_node_table;\n",
    "\"\"\"\n",
    "cur.execute(sql)\n",
    "records = cur.fetchall()\n",
    "\n",
    "for i in records:\n",
    "    \n",
    "    node_list[i[1]]=[\"netflow\",i[-2]+\":\"+i[-1]]\n",
    "\n",
    "net_uuid2hash={}\n",
    "for i in records:\n",
    "    net_uuid2hash[i[0]]=i[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_list_database=[]\n",
    "node_index=0\n",
    "for i in node_list:\n",
    "    node_list_database.append([i]+node_list[i]+[node_index])\n",
    "    node_index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''insert into node2id\n",
    "                     values %s\n",
    "        '''\n",
    "ex.execute_values(cur,sql, node_list_database,page_size=10000)\n",
    "connect.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing the map for nodeid to msg\n",
    "sql=\"select * from node2id ORDER BY index_id;\"\n",
    "cur.execute(sql)\n",
    "rows = cur.fetchall()\n",
    "\n",
    "nodeid2msg={}  # nodeid => msg and node hash => nodeid\n",
    "for i in rows:\n",
    "    nodeid2msg[i[0]]=i[-1]\n",
    "    nodeid2msg[i[-1]]={i[1]:i[2]} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodeid2msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalist=[]\n",
    "edge_type=set()\n",
    "reverse=[\"EVENT_ACCEPT\",\"EVENT_RECVFROM\",\"EVENT_RECVMSG\"]        \n",
    "for file in tqdm(filelist):\n",
    "        with open(filePath + file, \"r\") as f:\n",
    "            for line in (f):\n",
    "                if '{\"datum\":{\"com.bbn.tc.schema.avro.cdm18.Event\"' in line and \"EVENT_FLOWS_TO\" not in line:\n",
    "#                     print(line)\n",
    "                    subject_uuid=re.findall('\"subject\":{\"com.bbn.tc.schema.avro.cdm18.UUID\":\"(.*?)\"}',line)\n",
    "                    predicateObject_uuid=re.findall('\"predicateObject\":{\"com.bbn.tc.schema.avro.cdm18.UUID\":\"(.*?)\"}',line)\n",
    "                    if len(subject_uuid) >0 and len(predicateObject_uuid)>0:\n",
    "                        if subject_uuid[0] in subject_uuid2hash\\\n",
    "                        and (predicateObject_uuid[0] in file_uuid2hash or predicateObject_uuid[0] in net_uuid2hash):\n",
    "                            relation_type=re.findall('\"type\":\"(.*?)\"',line)[0]\n",
    "                            time_rec=re.findall('\"timestampNanos\":(.*?),',line)[0]\n",
    "                            time_rec=int(time_rec)\n",
    "                            subjectId=subject_uuid2hash[subject_uuid[0]]\n",
    "                            if predicateObject_uuid[0] in file_uuid2hash:\n",
    "                                objectId=file_uuid2hash[predicateObject_uuid[0]]\n",
    "                            else:\n",
    "                                objectId=net_uuid2hash[predicateObject_uuid[0]]\n",
    "#                                 print(line)\n",
    "                            edge_type.add(relation_type)\n",
    "                            if relation_type in reverse:\n",
    "                                datalist.append([objectId,nodeid2msg[objectId],relation_type,subjectId,nodeid2msg[subjectId],time_rec])\n",
    "                            else:\n",
    "                                datalist.append([subjectId,nodeid2msg[subjectId],relation_type,objectId,nodeid2msg[objectId],time_rec])\n",
    "\n",
    "               \n",
    "     \n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''insert into event_table\n",
    "                     values %s\n",
    "        '''\n",
    "ex.execute_values(cur,sql, datalist,page_size=10000)\n",
    "connect.commit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featurization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "FH_string=FeatureHasher(n_features=16,input_type=\"string\")\n",
    "FH_dict=FeatureHasher(n_features=16,input_type=\"dict\")\n",
    "\n",
    "\n",
    "def path2higlist(p):\n",
    "    l=[]\n",
    "    spl=p.strip().split('/')\n",
    "    for i in spl:\n",
    "        if len(l)!=0:\n",
    "            l.append(l[-1]+'/'+i)\n",
    "        else:\n",
    "            l.append(i)\n",
    "#     print(l)\n",
    "    return l\n",
    "\n",
    "def ip2higlist(p):\n",
    "    l=[]\n",
    "    spl=p.strip().split('.')\n",
    "    for i in spl:\n",
    "        if len(l)!=0:\n",
    "            l.append(l[-1]+'.'+i)\n",
    "        else:\n",
    "            l.append(i)\n",
    "#     print(l)\n",
    "    return l\n",
    "\n",
    "\n",
    "def subject2higlist(p):\n",
    "    l=[]\n",
    "    spl=p.strip().split('.')\n",
    "    for i in spl:\n",
    "        if len(l)!=0:\n",
    "            l.append(l[-1]+'.'+i)\n",
    "        else:\n",
    "            l.append(i)\n",
    "#     print(l)\n",
    "    return l\n",
    "\n",
    "\n",
    "def list2str(l):\n",
    "    s=''\n",
    "    for i in l:\n",
    "        s+=i\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_msg_vec=[]\n",
    "node_msg_dic_list=[]\n",
    "for i in tqdm(nodeid2msg.keys()):\n",
    "    if type(i)==int:\n",
    "        if 'netflow' in nodeid2msg[i].keys():\n",
    "            higlist=['netflow']\n",
    "            higlist+=ip2higlist(nodeid2msg[i]['netflow'])\n",
    "            \n",
    "        if 'file' in nodeid2msg[i].keys():\n",
    "            higlist=['file']\n",
    "            higlist+=path2higlist(nodeid2msg[i]['file'])\n",
    "            \n",
    "#             print(higlist)\n",
    "        if 'subject' in nodeid2msg[i].keys():\n",
    "            higlist=['subject']\n",
    "            higlist+=subject2higlist(nodeid2msg[i]['subject'])\n",
    "        node_msg_dic_list.append(list2str(higlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node2higvec=[]\n",
    "for i in tqdm(node_msg_dic_list):\n",
    "    vec=FH_string.transform([i]).toarray()\n",
    "    node2higvec.append(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node2higvec=np.array(node2higvec).reshape([-1,16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel2id={1: 'EVENT_CLOSE',\n",
    " 'EVENT_CLOSE': 1,\n",
    " 2: 'EVENT_OPEN',\n",
    " 'EVENT_OPEN': 2,\n",
    " 3: 'EVENT_READ',\n",
    " 'EVENT_READ': 3,\n",
    " 4: 'EVENT_WRITE',\n",
    " 'EVENT_WRITE': 4,\n",
    " 5: 'EVENT_RECVFROM',\n",
    " 'EVENT_RECVFROM': 5,\n",
    " 6: 'EVENT_RECVMSG',\n",
    " 'EVENT_RECVMSG': 6,\n",
    " 7: 'EVENT_SENDMSG',\n",
    " 'EVENT_SENDMSG': 7,\n",
    " 8: 'EVENT_SENDTO',\n",
    " 'EVENT_SENDTO': 8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geneate edge type one-hot\n",
    "relvec=torch.nn.functional.one_hot(torch.arange(0, len(rel2id.keys())//2), num_classes=len(rel2id.keys())//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map different relation types to their one-hot encoding\n",
    "rel2vec={}\n",
    "for i in rel2id.keys():\n",
    "    if type(i) is not int:\n",
    "        rel2vec[i]= relvec[rel2id[i]-1]\n",
    "        rel2vec[relvec[rel2id[i]-1]]=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results\n",
    "torch.save(node2higvec,\"node2higvec\")\n",
    "torch.save(rel2vec,\"rel2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node2higvec=torch.load(\"./node2higvec\")\n",
    "rel2vec=torch.load(\"./rel2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"mkdir -p ./train_graphs/\")\n",
    "for day in tqdm(range(2,14)):\n",
    "    start_timestamp=datetime_to_ns_time_US('2018-04-'+str(day)+' 00:00:00')\n",
    "    end_timestamp=datetime_to_ns_time_US('2018-04-'+str(day+1)+' 00:00:00')\n",
    "    sql=\"\"\"\n",
    "    select * from event_table\n",
    "    where\n",
    "          timestamp_rec>'%s' and timestamp_rec<'%s'\n",
    "           ORDER BY timestamp_rec;\n",
    "    \"\"\"%(start_timestamp,end_timestamp)\n",
    "    cur.execute(sql)\n",
    "    events = cur.fetchall()\n",
    "    print('2018-04-'+str(day),\" events count:\",str(len(events)))\n",
    "    edge_list=[]\n",
    "    for e in events:\n",
    "        edge_temp=[int(e[1]),int(e[4]),e[2],e[5]]\n",
    "        if e[2] in rel2id:\n",
    "#         if True:\n",
    "            edge_list.append(edge_temp)\n",
    "    print('2018-04-'+str(day),\" edge list len:\",str(len(edge_list)))\n",
    "\n",
    "    dataset = TemporalData()\n",
    "    src = []\n",
    "    dst = []\n",
    "    msg = []\n",
    "    t = []\n",
    "    for i in edge_list:\n",
    "        src.append(int(i[0]))\n",
    "        dst.append(int(i[1]))\n",
    "    #     msg.append(torch.cat([torch.from_numpy(node2higvec_bn[i[0]]), rel2vec[i[2]], torch.from_numpy(node2higvec_bn[i[1]])] ))\n",
    "        msg.append(torch.cat([torch.from_numpy(node2higvec[i[0]]), rel2vec[i[2]], torch.from_numpy(node2higvec[i[1]])] ))\n",
    "        t.append(int(i[3]))\n",
    "\n",
    "    dataset.src = torch.tensor(src)\n",
    "    dataset.dst = torch.tensor(dst)\n",
    "    dataset.t = torch.tensor(t)\n",
    "    dataset.msg = torch.vstack(msg)\n",
    "    dataset.src = dataset.src.to(torch.long)\n",
    "    dataset.dst = dataset.dst.to(torch.long)\n",
    "    dataset.msg = dataset.msg.to(torch.float)\n",
    "    dataset.t = dataset.t.to(torch.long)\n",
    "    torch.save(dataset, \"./train_graphs/graph_4_\"+str(day)+\".TemporalData.simple\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kairos",
   "language": "python",
   "name": "kairos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "197.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
